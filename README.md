# Terafac Image Classification Challenge
### Stanford Cars196 â€“ Multi-Level Deep Learning Solution

## Overview
This repository contains my submission for the **Terafac Image Classification Challenge**, implemented on the **Stanford Cars196** dataset.  
The task focuses on **fine-grained image classification**, where visually similar car models must be distinguished accurately.

The work progresses systematically from a **baseline transfer-learning model (Level 1)** to an **expert-level ensemble system (Level 4)**, emphasizing not only accuracy but also **reasoning, reproducibility, interpretability, and analysis**, as outlined in the Terafac evaluation rubric.

---
## Problem Understanding
Fine-grained image classification is challenging because:
- Classes are **visually very similar** (e.g., different car models of the same brand)
- Discriminative cues are often **localized** (headlights, grills, logos)
- Background clutter can mislead the model
- Dataset may contain **noise and class imbalance**

The goal was not only to achieve high accuracy, but to:
- Build a clean and reproducible pipeline
- Analyze failures and limitations
- Apply advanced techniques where they genuinely help

---

## Dataset
- **Dataset:** Stanford Cars196  
- **Source:** Official Stanford Cars Dataset  
- **Total Classes:** 196  
- **Annotations:** Bounding boxes + class labels

### Dataset Source
The dataset was obtained from the official Stanford Cars196 release and loaded using the provided annotation `.mat` files (`cars_train_annos.mat`, `cars_test_annos.mat`).

### Bounding Box Usage
Bounding box annotations were used to **crop the vehicle region** before training.  
This reduced background noise and helped the model focus on fine-grained vehicle features.

---

## Dataset Split Strategy (Mandatory Compliance)
To strictly follow Terafacâ€™s requirements:
## Problem Understanding
Fine-grained image classification is challenging because:
- Classes are **visually very similar** (e.g., different car models of the same brand)
- Discriminative cues are often **localized** (headlights, grills, logos)
- Background clutter can mislead the model
- Dataset may contain **noise and class imbalance**

The goal was not only to achieve high accuracy, but to:
- Build a clean and reproducible pipeline
- Analyze failures and limitations
- Apply advanced techniques where they genuinely help

---

## Dataset
- **Dataset:** Stanford Cars196  
- **Source:** Official Stanford Cars Dataset  
- **Total Classes:** 196  
- **Annotations:** Bounding boxes + class labels

### Dataset Source
The dataset was obtained from the official Stanford Cars196 release and loaded using the provided annotation `.mat` files (`cars_train_annos.mat`, `cars_test_annos.mat`).

### Bounding Box Usage
Bounding box annotations were used to **crop the vehicle region** before training.  
This reduced background noise and helped the model focus on fine-grained vehicle features.

---

## Dataset Split Strategy (Mandatory Compliance)
To strictly follow Terafacâ€™s requirements:
Train : 80%
Validation : 10%
Test : 10%


- The official training annotations were used.
- Validation data was **derived from the training set** using **stratified sampling** to preserve class balance.
- The official test split was kept separate.
- This maintains an effective **80-10-10 distribution**, as required.

---

## Level-wise Implementation Summary

### ðŸ”¹ Level 1 â€“ Baseline Model
**Objective:** Establish a strong baseline using transfer learning.

- Model: EfficientNet-B4 (pretrained on ImageNet)
- Input: Cropped vehicle images
- Loss: Cross-Entropy
- Outcome: Strong baseline accuracy with clean training pipeline

This level ensured correct data loading, splitting, and reproducibility.

---

### ðŸ”¹ Level 2 â€“ Intermediate Improvements
**Objective:** Improve generalization through systematic enhancements.

Techniques applied:
- Data augmentation (horizontal flip, color jitter)
- Label smoothing
- AdamW optimizer with cosine learning-rate scheduling

**Observation:**  
These techniques improved validation stability and reduced overfitting compared to the baseline.

---

### ðŸ”¹ Level 3 â€“ Advanced Architecture & Interpretability
**Objective:** Demonstrate architectural reasoning and interpretability.

Key additions:
- Fine-tuning EfficientNet with bounding-box cropping
- Grad-CAM visualizations for interpretability
- Per-class and qualitative error analysis

**Findings:**
- The model focuses on meaningful regions (headlights, grills, wheels)
- Misclassifications often occur between visually near-identical models
- Interpretability helped validate that learning was meaningful, not spurious

---

### ðŸ”¹ Level 4 â€“ Expert Techniques (Ensemble Learning)
**Objective:** Improve robustness using ensemble methods (shortlist threshold).

Instead of meta-learning or reinforcement learning, **ensemble learning** was chosen due to:
- Practical effectiveness
- Stability
- Industry relevance

#### Models Used
- EfficientNet-B4
- ConvNeXt-Tiny

#### Ensemble Strategy
- **Soft-voting ensemble**
- Averaging class-probability outputs from both models
- Final prediction via argmax of averaged probabilities

#### Results
| Model | Validation Accuracy |
|------|---------------------|
| EfficientNet-B4 | ~92% |
| ConvNeXt-Tiny | ~92.9% |
| **Ensemble (Soft Voting)** | **92.33%** |

The ensemble improves robustness and reduces model-specific errors, even when raw accuracy gains are modest.

---

## Key Challenges & Failures
- Some car models remain difficult due to **extreme visual similarity**
- Increasing model capacity alone led to overfitting
- Gains beyond ~93% require disproportionate complexity

These limitations were acknowledged rather than hidden, and analysis was prioritized over aggressive tuning.

---

## Repository Structure
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ level_1/
â”‚ â””â”€â”€ level_1_baseline.ipynb
â”œâ”€â”€ level_2/
â”‚ â””â”€â”€ level_2_improvements.ipynb
â”œâ”€â”€ level_3/
â”‚ â””â”€â”€ level_3_advanced_architecture.ipynb
â”œâ”€â”€ level_4/
â”‚ â””â”€â”€ level_4_ensemble.ipynb
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ efficientnet_cars196_final.pth
â”‚ â”œâ”€â”€ convnext_tiny_best.pth
â”‚ â””â”€â”€ checkpoint.pth
â”‚
â”œâ”€â”€ results/
â”‚ â”œâ”€â”€ training_curves.png
â”‚ â”œâ”€â”€ confusion_matrix.png
â”‚ â”œâ”€â”€ gradcam_example.png
â”‚ â””â”€â”€ ensemble_val_predictions.csv
â”‚
â””â”€â”€ report/
â””â”€â”€ terafac_level4_report.pdf 

---

## How to Run
1. Clone the repository
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
3. Open notebooks level-wise (Level 1 â†’ Level 4)

4. GPU is recommended for training and ensemble inference

Google Colab Notebooks (Mandatory)

Note: All notebooks were developed and executed in Google Colab.

Level 1 & Level 2 â€“ Baseline and Improvents: (Colab link : https://colab.research.google.com/drive/1w8Jdg8meecvq9OfgnyvfAr7ALMmhjvMU?usp=sharing)


Level 3 â€“ Advanced Architecture: (Colab link : https://colab.research.google.com/drive/1vChRNnTdASqLYRz0hMbwMCAi-Lp--8QV?usp=sharing)


Level 4 â€“ Ensemble: (Colab link : https://colab.research.google.com/drive/1znj6R29-tYvdYBBTXxu3KTiCwI7rnyRF?usp=sharing and Colab Link : https://colab.research.google.com/drive/1Tc0kDd5DTSA_9w_8nGdwbzLjJ7Qtid6Y?usp=sharing) 

Key Learnings

Clean data handling matters more than aggressive modeling

Bounding-box cropping significantly improves fine-grained recognition

Ensemble methods improve robustness even when accuracy gains are small

Interpretability tools are essential for validating real learning